{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e687c5ad",
   "metadata": {},
   "source": [
    "# Evaluación de nuestro RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3bb1ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from classes.chatbot import Chatbot\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ceb92",
   "metadata": {},
   "source": [
    "El modelo que vamos a usar para las evaluaciones y nuestro RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4eeebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-small-latest\", \n",
    "    mistral_api_key=MISTRAL_API_KEY,\n",
    "    temperature=0, \n",
    "    random_seed=12345\n",
    ")\n",
    "\n",
    "search_type = \"similarity\" # [\"similarity\", \"mmr\", \"tfidf\", \"bm25\", \"grafo\"]\n",
    "chroma_directory = \"chroma_nomic_512\"  \n",
    "\n",
    "chatbot = Chatbot(\n",
    "    chroma_directory=chroma_directory,  # directorio de la base de datos\n",
    "    search_type=search_type,            # tipo de búsqueda en la base de datos\n",
    "    k=10,                               # documentos recuperados de la base de datos\n",
    "    top_n=5                             # documentos recuperados por el reranker\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f21db7d",
   "metadata": {},
   "source": [
    "Cargamos una muestra de nuestro dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c37cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"eval/test - preguntas_mistral_plus.csv\", on_bad_lines=\"skip\")\n",
    "df_shuffle = df.sample(frac=1, random_state=12345)\n",
    "# test_sample = df_shuffle.head(10)\n",
    "test_sample = df\n",
    "# df_shuffle.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180055f9",
   "metadata": {},
   "source": [
    "## Configuración de los agentes críticos\n",
    "\n",
    "- **Accuracy**: Grado de precisión de la respuesta generada\n",
    "- **Faithfulness**: Si la respuesta se basa exclusivamente en los documentos recuperados\n",
    "- **Groundedness**: Si la pregunta puede ser respondida con el contexto aportado\n",
    "- **Relevance**: Si los docs recuperados tienen que ver con la pregunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcd0fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_prompt = \"\"\"\n",
    "Eres un agente crítico experto en evaluación de precisión de respuestas. \n",
    "Evalúa la precisión de la respuesta generada en comparación con la respuesta esperada. \n",
    "\n",
    "Criterios de evaluación:\n",
    "- 5: La respuesta generada incluye toda la información esencial de la respuesta esperada, incluso si es más extensa o utiliza otras palabras.\n",
    "- 4: La respuesta generada incluye casi toda la información importante, con leves omisiones o diferencias menores que no afectan el significado general.\n",
    "- 3: La respuesta generada cubre parte significativa del contenido, pero falta información importante o hay errores leves.\n",
    "- 2: La respuesta generada solo refleja una parte limitada del contenido de la respuesta esperada y tiene omisiones o distorsiones relevantes.\n",
    "- 1: La respuesta generada tiene muy poca relación con la respuesta esperada, presenta errores graves u omisiones importantes.\n",
    "- 0: La respuesta generada no contiene información relevante de la respuesta esperada o está completamente incorrecta.\n",
    "\n",
    "Recuerda: una redacción diferente o más extensa no debe penalizar si toda la información está presente.\n",
    "\n",
    "respuesta esperada: {respuesta_esperada}\n",
    "\n",
    "respuesta generada: {respuesta_generada}\n",
    "\n",
    "Responde solo con tu evaluación en el siguiente formato:\n",
    "\n",
    "Respuesta:::\n",
    "Rating: (tu evaluación del 0 al 5)\n",
    "Respuesta:::\n",
    "\"\"\"\n",
    "\n",
    "faithfulness_prompt = \"\"\"\n",
    "Tu tarea es evaluar si la respuesta generada usa EXCLUSIVAMENTE información contenida en los documentos proporcionados, sin añadir datos externos, suposiciones infundadas ni invenciones.\n",
    "\n",
    "Criterios de evaluación:\n",
    "5 - Totalmente basada en los documentos, sin añadir nada externo.\n",
    "4 - Principalmente basada, con pequeñas generalizaciones o frases neutras.\n",
    "3 - Incluye inferencias plausibles pero no explícitas en los documentos.\n",
    "2 - Mezcla partes basadas en los documentos con invenciones o suposiciones.\n",
    "1 - Mayormente inventada, aunque con algún detalle correcto.\n",
    "0 - Completamente inventada, contradictoria o irrelevante.\n",
    "\n",
    "Aquí tienes los documentos recuperados y la respuesta generada por el sistema RAG.\n",
    "\n",
    "Documentos recuperados: {documentos}\n",
    "\n",
    "Respuesta generada: {respuesta}\n",
    "\n",
    "Responde solo con tu evaluación en el siguiente formato:\n",
    "\n",
    "Respuesta:::\n",
    "Rating: (tu evaluación del 0 al 5)\n",
    "Respuesta:::\n",
    "\"\"\"\n",
    "\n",
    "groundedness_prompt = \"\"\"\n",
    "Tu tarea es evaluar en qué medida la respuesta está respaldada por evidencia específica de los documentos proporcionados.\n",
    "\n",
    "Criterios de evaluación:\n",
    "5 - Usa varias evidencias clave con precisión y contexto adecuado.\n",
    "4 - Usa al menos una evidencia clave de forma clara y correcta.\n",
    "3 - Se refiere a los documentos de manera vaga o genérica.\n",
    "2 - Uso superficial, indirecto o débilmente relevante de los documentos.\n",
    "1 - Menciona información incorrecta o mal interpretada del documento.\n",
    "0 - No hace uso de los documentos; ignora la evidencia disponible.\n",
    "\n",
    "Aquí tienes los documentos recuperados y la respuesta generada por el sistema RAG.\n",
    "\n",
    "Documentos recuperados: {documentos}\n",
    "\n",
    "Respuesta generada: {respuesta}\n",
    "\n",
    "Responde solo con tu evaluación en el siguiente formato:\n",
    "\n",
    "Respuesta:::\n",
    "Rating: (tu evaluación del 0 al 5)\n",
    "Respuesta:::\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "relevance_prompt = \"\"\"\n",
    "Tu tarea es evaluar qué tan relevantes son los documentos para responder la pregunta.\n",
    "\n",
    "Criterios de evaluación:\n",
    "5 - Abordan directamente el núcleo de la pregunta\n",
    "4 - Cubren aspectos importantes pero no todos\n",
    "3 - Proporcionan contexto general útil\n",
    "2 - Tienen conexión tangencial con el tema\n",
    "1 - Mínima relación con la pregunta\n",
    "0 - Completamente irrelevantes\n",
    "\n",
    "Aquí tienes la pregunta y los documentos recuperados.\n",
    "\n",
    "Pregunta: {pregunta}\n",
    "\n",
    "Respuesta esperada: {respuesta}\n",
    "\n",
    "Documentos recuperados: {documentos}\n",
    "\n",
    "Responde solo con tu evaluación en el siguiente formato:\n",
    "\n",
    "Respuesta:::\n",
    "Rating: (tu evaluación del 0 al 5)\n",
    "Respuesta:::\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3341c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(question: str, chatbot: Chatbot):\n",
    "    results = chatbot.get_retriever().invoke(question)\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in results]) \n",
    "    \n",
    "    response = chatbot.answer_query2(question, context)\n",
    "\n",
    "    return response, context\n",
    "\n",
    "def get_response_reranker(question: str, chatbot: Chatbot):\n",
    "    results = chatbot.get_compression_retriever().invoke(question)\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in results]) \n",
    "    \n",
    "    response = chatbot.answer_query2(question, context)\n",
    "\n",
    "    return response, context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b35e32",
   "metadata": {},
   "source": [
    "Evaluación del RAG + Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3d333a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando el RAG:   0%|          | 0/312 [00:04<?, ?pregunta/s]\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m### NORMAL ###\u001b[39;00m\n\u001b[32m     19\u001b[39m time.sleep(sleep) \u001b[38;5;66;03m# Espera para evitar sobrecargar el servidor\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m respuesta_generada, docs_recuperados = \u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpregunta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchatbot\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# print(docs_recuperados)\u001b[39;00m\n\u001b[32m     23\u001b[39m evaluations = {\n\u001b[32m     24\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: accuracy_prompt.format(respuesta_esperada=respuesta, respuesta_generada=respuesta_generada),\n\u001b[32m     25\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfaithfulness\u001b[39m\u001b[33m\"\u001b[39m: faithfulness_prompt.format(documentos=docs_recuperados, respuesta=respuesta_generada),\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mgroundedness\u001b[39m\u001b[33m\"\u001b[39m: groundedness_prompt.format(documentos=docs_recuperados, respuesta=respuesta_generada),\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrelevance\u001b[39m\u001b[33m\"\u001b[39m: relevance_prompt.format(pregunta=pregunta, respuesta=respuesta, documentos=docs_recuperados)\n\u001b[32m     28\u001b[39m }        \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mget_response\u001b[39m\u001b[34m(question, chatbot)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_response\u001b[39m(question: \u001b[38;5;28mstr\u001b[39m, chatbot: Chatbot):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     results = \u001b[43mchatbot\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     context = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m results]) \n\u001b[32m      5\u001b[39m     response = chatbot.answer_query2(question, context)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositorios\\TFM\\.venv\\Lib\\site-packages\\langchain_core\\retrievers.py:263\u001b[39m, in \u001b[36mBaseRetriever.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._get_relevant_documents(\n\u001b[32m    260\u001b[39m             \u001b[38;5;28minput\u001b[39m, run_manager=run_manager, **_kwargs\n\u001b[32m    261\u001b[39m         )\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    265\u001b[39m     run_manager.on_retriever_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositorios\\TFM\\.venv\\Lib\\site-packages\\langchain_graph_retriever\\graph_retriever.py:132\u001b[39m, in \u001b[36mGraphRetriever._get_relevant_documents\u001b[39m\u001b[34m(self, query, edges, initial_roots, filter, store_kwargs, **kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m edges \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33medges\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must be provided in this call or the constructor\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m nodes = \u001b[43mtraverse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43medges\u001b[49m\u001b[43m=\u001b[49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStrategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madapter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_root_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_roots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstore_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstore_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [node_to_doc(n) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositorios\\TFM\\.venv\\Lib\\site-packages\\graph_retriever\\traversal.py:63\u001b[39m, in \u001b[36mtraverse\u001b[39m\u001b[34m(query, edges, strategy, store, metadata_filter, initial_root_ids, store_kwargs)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[33;03mPerform a graph traversal to retrieve nodes for a specific query.\u001b[39;00m\n\u001b[32m     27\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m \u001b[33;03m    Nodes returned by the traversal.\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     54\u001b[39m traversal = _Traversal(\n\u001b[32m     55\u001b[39m     query=query,\n\u001b[32m     56\u001b[39m     edges=edges,\n\u001b[32m   (...)\u001b[39m\u001b[32m     61\u001b[39m     store_kwargs=store_kwargs,\n\u001b[32m     62\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtraversal\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositorios\\TFM\\.venv\\Lib\\site-packages\\graph_retriever\\traversal.py:183\u001b[39m, in \u001b[36m_Traversal.traverse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28mself\u001b[39m._check_first_use()\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# Retrieve initial candidates.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m initial_content = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_initial_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.initial_root_ids:\n\u001b[32m    185\u001b[39m     initial_content.extend(\u001b[38;5;28mself\u001b[39m.store.get(\u001b[38;5;28mself\u001b[39m.initial_root_ids))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositorios\\TFM\\.venv\\Lib\\site-packages\\graph_retriever\\traversal.py:245\u001b[39m, in \u001b[36m_Traversal._fetch_initial_candidates\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fetch_initial_candidates\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Content]:\n\u001b[32m    237\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[33;03m    Retrieve initial candidates based on the query and strategy.\u001b[39;00m\n\u001b[32m    239\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m \u001b[33;03m        The initial content retrieved via similarity search.\u001b[39;00m\n\u001b[32m    244\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m     query_embedding, docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch_with_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetadata_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstore_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28mself\u001b[39m.strategy._query_embedding = query_embedding\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositorios\\TFM\\.venv\\Lib\\site-packages\\langchain_graph_retriever\\adapters\\langchain.py:100\u001b[39m, in \u001b[36mLangchainAdapter.search_with_embedding\u001b[39m\u001b[34m(self, query, k, filter, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msearch_with_embedding\u001b[39m(\n\u001b[32m     94\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     98\u001b[39m     **kwargs: Any,\n\u001b[32m     99\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m], \u001b[38;5;28mlist\u001b[39m[Content]]:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     query_embedding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m     docs = \u001b[38;5;28mself\u001b[39m.search(\n\u001b[32m    102\u001b[39m         embedding=query_embedding,\n\u001b[32m    103\u001b[39m         k=k,\n\u001b[32m    104\u001b[39m         \u001b[38;5;28mfilter\u001b[39m=\u001b[38;5;28mfilter\u001b[39m,\n\u001b[32m    105\u001b[39m         **kwargs,\n\u001b[32m    106\u001b[39m     )\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m query_embedding, docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositorios\\TFM\\.venv\\Lib\\site-packages\\langchain_graph_retriever\\adapters\\langchain.py:52\u001b[39m, in \u001b[36mLangchainAdapter.embed_query\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m     51\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the embedding of the query.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_safe_embedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositorios\\TFM\\.venv\\Lib\\site-packages\\langchain_ollama\\embeddings.py:244\u001b[39m, in \u001b[36mOllamaEmbeddings.embed_query\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    243\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Embed query text.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositorios\\TFM\\.venv\\Lib\\site-packages\\langchain_ollama\\embeddings.py:237\u001b[39m, in \u001b[36mOllamaEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) -> List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m    236\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Embed search docs.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     embedded_docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_default_params\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m embedded_docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositorios\\TFM\\.venv\\Lib\\site-packages\\ollama\\_client.py:357\u001b[39m, in \u001b[36mClient.embed\u001b[39m\u001b[34m(self, model, input, truncate, options, keep_alive)\u001b[39m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed\u001b[39m(\n\u001b[32m    350\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    351\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    355\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    356\u001b[39m ) -> EmbedResponse:\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mEmbedResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/embed\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEmbedRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositorios\\TFM\\.venv\\Lib\\site-packages\\ollama\\_client.py:178\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    176\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Repositorios\\TFM\\.venv\\Lib\\site-packages\\ollama\\_client.py:124\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mConnectionError\u001b[39m: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download"
     ]
    }
   ],
   "source": [
    "columns = [\n",
    "    \"pregunta\", \"respuesta esperada\", \"respuesta generada\", \n",
    "    \"accuracy\", \"faithfulness\", \n",
    "    \"groundedness\", \"relevance\"\n",
    "]\n",
    "\n",
    "eval_rag = []\n",
    "eval_rag_rr = []\n",
    "\n",
    "sleep = 2\n",
    "\n",
    "for index, row in tqdm(test_sample.iterrows(), desc=\"Evaluando el RAG\", total=len(test_sample), unit=\"pregunta\"):\n",
    "    pregunta = row[\"Pregunta\"]\n",
    "    respuesta = row[\"Respuesta\"]\n",
    "    contexto = row[\"Contexto\"]       \n",
    "\n",
    "    ### NORMAL ###\n",
    "\n",
    "    time.sleep(sleep) # Espera para evitar sobrecargar el servidor\n",
    "    respuesta_generada, docs_recuperados = get_response(pregunta, chatbot)  \n",
    "    # print(docs_recuperados)\n",
    "\n",
    "    evaluations = {\n",
    "        \"accuracy\": accuracy_prompt.format(respuesta_esperada=respuesta, respuesta_generada=respuesta_generada),\n",
    "        \"faithfulness\": faithfulness_prompt.format(documentos=docs_recuperados, respuesta=respuesta_generada),\n",
    "        \"groundedness\": groundedness_prompt.format(documentos=docs_recuperados, respuesta=respuesta_generada),\n",
    "        \"relevance\": relevance_prompt.format(pregunta=pregunta, respuesta=respuesta, documentos=docs_recuperados)\n",
    "    }        \n",
    "\n",
    "\n",
    "    ### RERANKING ###\n",
    "    time.sleep(sleep)\n",
    "    respuesta_generada_rr, docs_recuperados_rr = get_response_reranker(pregunta, chatbot)   \n",
    "    # print(docs_recuperados)\n",
    "\n",
    "    evaluations_rr = {\n",
    "        \"accuracy\": accuracy_prompt.format(respuesta_esperada=respuesta, respuesta_generada=respuesta_generada_rr),\n",
    "        \"faithfulness\": faithfulness_prompt.format(documentos=docs_recuperados_rr, respuesta=respuesta_generada_rr),\n",
    "        \"groundedness\": groundedness_prompt.format(documentos=docs_recuperados_rr, respuesta=respuesta_generada_rr),\n",
    "        \"relevance\": relevance_prompt.format(pregunta=pregunta, respuesta=respuesta, documentos=docs_recuperados_rr)\n",
    "    } \n",
    "\n",
    "    try:\n",
    "\n",
    "        eval_row = {\n",
    "            \"pregunta\": pregunta,\n",
    "            \"respuesta esperada\": respuesta,\n",
    "            \"respuesta generada\": respuesta_generada            \n",
    "        }\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            time.sleep(sleep) \n",
    "            chain = llm | StrOutputParser()\n",
    "            response = chain.invoke(evaluation)\n",
    "            match = re.search(r'Rating:\\s*(\\d+)', response)\n",
    "            score = int(match.group(1))\n",
    "            eval_row[criterion] = score\n",
    "        eval_rag.append(eval_row)\n",
    "\n",
    "        #####################\n",
    "\n",
    "        eval_row_rr = {\n",
    "            \"pregunta\": pregunta,\n",
    "            \"respuesta esperada\": respuesta,\n",
    "            \"respuesta generada\": respuesta_generada_rr            \n",
    "        }\n",
    "        for criterion, evaluation in evaluations_rr.items():\n",
    "            time.sleep(sleep) \n",
    "            chain = llm | StrOutputParser()\n",
    "            response = chain.invoke(evaluation)\n",
    "            match = re.search(r'Rating:\\s*(\\d+)', response)\n",
    "            score = int(match.group(1))\n",
    "            eval_row_rr[criterion] = score\n",
    "        eval_rag_rr.append(eval_row_rr)\n",
    "            \n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87930c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rag_df = pd.DataFrame(eval_rag, columns=columns)\n",
    "eval_rag_df.to_csv(f\"eval/eval_{search_type}.csv\", index=False)\n",
    "\n",
    "# print(\"Resultados RAG\")\n",
    "# eval_rag_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fb75be",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rag_df_rr = pd.DataFrame(eval_rag_rr, columns=columns)\n",
    "eval_rag_df_rr.to_csv(f\"eval/eval_{search_type}_rr.csv\", index=False)\n",
    "\n",
    "# print(\"Resultados RAG + rerank\")\n",
    "# eval_rag_df_rr.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
