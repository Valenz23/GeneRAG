{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e687c5ad",
   "metadata": {},
   "source": [
    "# Evaluación de nuestro RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3bb1ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from classes.chatbot import Chatbot\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ceb92",
   "metadata": {},
   "source": [
    "El modelo que vamos a usar para las evaluaciones y nuestro RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a4eeebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-small-latest\", \n",
    "    mistral_api_key=MISTRAL_API_KEY,\n",
    "    temperature=0, \n",
    "    random_seed=12345\n",
    ")\n",
    "\n",
    "search_type = \"similarity\" # [\"similarity\", \"mmr\", \"tfidf\", \"bm25\", \"grafo\"]\n",
    "\n",
    "chatbot = Chatbot(\n",
    "    chroma_directory=\"chroma\",  # directorio de la base de datos\n",
    "    search_type=search_type,    # tipo de búsqueda en la base de datos\n",
    "    k=10,                        # documentos recuperados de la base de datos\n",
    "    top_n=5                     # documentos recuperados por el reranker\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f21db7d",
   "metadata": {},
   "source": [
    "Cargamos una muestra de nuestro dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c37cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"eval/test - preguntas_mistral_plus.csv\", on_bad_lines=\"skip\")\n",
    "df_shuffle = df.sample(frac=1, random_state=12345)\n",
    "# test_sample = df_shuffle.head(50)\n",
    "test_sample = df\n",
    "# df_shuffle.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180055f9",
   "metadata": {},
   "source": [
    "## Configuración de los agentes críticos\n",
    "\n",
    "- **Accuracy**: Grado de acierto de la respuesta final\n",
    "- **Faithfulness**: Si la respuesta se basa exclusivamente en los documentos recuperados\n",
    "- **Groundedness**: Si la pregunta puede ser respondida con el contexto aportado\n",
    "- **Relevance**: Si los docs recuperados tienen que ver con la pregunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcd0fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_prompt = \"\"\"\n",
    "Tu tarea es evaluar la precisión de la respuesta generada comparándola con la respuesta esperada.\n",
    "\n",
    "Criterios de evaluación:\n",
    "5 - Exactamente igual en contenido y significado\n",
    "4 - Pequeñas diferencias irrelevantes\n",
    "3 - Contiene algunos errores menores pero el mensaje principal es correcto\n",
    "2 - Diferencias significativas pero algo de información correcta\n",
    "1 - Mínima relación con la respuesta esperada\n",
    "0 - Totalmente incorrecta o contradictoria\n",
    "\n",
    "Provee tu respuesta como sigue:\n",
    "\n",
    "Respuesta:::\n",
    "Rating: X\n",
    "\n",
    "----\n",
    "Respuesta esperada: {respuesta_esperada}\\n\n",
    "Respuesta generada: {respuesta_generada}\\n\n",
    "Respuesta:::\n",
    "\"\"\"\n",
    "\n",
    "faithfulness_prompt = \"\"\"\n",
    "Tu tarea es evaluar si la respuesta usa SOLO información de los documentos.\n",
    "\n",
    "Criterios:\n",
    "5 - Totalmente basada en los documentos (sin añadidos)\n",
    "4 - Principalmente basada, con frases genéricas inocuas\n",
    "3 - Incluye inferencias razonables no explícitas\n",
    "2 - Mezcla información documentada con suposiciones\n",
    "1 - Mayormente inventada pero con algún detalle relevante\n",
    "0 - Totalmente inventada o irrelevante\n",
    "\n",
    "Provee tu respuesta como sigue:\n",
    "\n",
    "Respuesta:::\n",
    "Rating: X\n",
    "\n",
    "Respuesta:::\n",
    "Rating: (tu rating, como número del 0 al 5)\n",
    "\n",
    "Aqui tienes los documentos recuperados y la respuesta generada por el sistema RAG.\n",
    "\n",
    "Documentos recuperados: {documentos}\\n\n",
    "Respuesta generada: {respuesta}\\n\n",
    "Respuesta:::\n",
    "\"\"\"\n",
    "\n",
    "groundedness_prompt = \"\"\"\n",
    "Tu tarea es evaluar cómo de bien la respuesta usa la evidencia de los documentos.\n",
    "\n",
    "Criterios:\n",
    "5 - Usa múltiples evidencias clave de forma precisa\n",
    "4 - Usa al menos una evidencia clave correctamente\n",
    "3 - Menciona los documentos pero de forma vaga\n",
    "2 - Uso superficial o poco relevante de los documentos\n",
    "1 - Cita documentos incorrectamente\n",
    "0 - Ignora por completo los documentos\n",
    "\n",
    "Provee tu respuesta como sigue:\n",
    "\n",
    "Respuesta:::\n",
    "Rating: (tu rating, como número del 0 al 5)\n",
    "\n",
    "Aqui tienes los documentos recuperados y la respuesta generada por el sistema RAG.\n",
    "\n",
    "Documentos recuperados: {documentos}\\n\n",
    "Respuesta generada: {respuesta}\\n\n",
    "Respuesta:::\n",
    "\"\"\"\n",
    "\n",
    "relevance_prompt = \"\"\"\n",
    "Tu tarea es evaluar qué tan relevantes son los documentos para responder la pregunta.\n",
    "\n",
    "Criterios:\n",
    "5 - Abordan directamente el núcleo de la pregunta\n",
    "4 - Cubren aspectos importantes pero no todos\n",
    "3 - Proporcionan contexto general útil\n",
    "2 - Tienen conexión tangencial con el tema\n",
    "1 - Mínima relación con la pregunta\n",
    "0 - Completamente irrelevantes\n",
    "\n",
    "Provee tu respuesta como sigue:\n",
    "\n",
    "Aquí tienes la pregunta y los documentos recuperados.\n",
    "\n",
    "Respuesta:::\n",
    "Rating: (tu rating, como número del 0 al 5)\n",
    "\n",
    "Pregunta: {pregunta}\\n\n",
    "Respuesta esperada: {respuesta}\\n\n",
    "Documentos recuperados: {documentos}\\n\n",
    "Respuesta:::\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3341c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(question: str, chatbot: Chatbot):\n",
    "    results = chatbot.get_retriever().invoke(question)\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in results]) \n",
    "    \n",
    "    response = chatbot.answer_query2(question, context)\n",
    "\n",
    "    return response, context\n",
    "\n",
    "def get_response_reranker(question: str, chatbot: Chatbot):\n",
    "    results = chatbot.get_compression_retriever().invoke(question)\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in results]) \n",
    "    \n",
    "    response = chatbot.answer_query2(question, context)\n",
    "\n",
    "    return response, context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b35e32",
   "metadata": {},
   "source": [
    "Evaluación del RAG + Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3d333a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando el RAG: 100%|██████████| 312/312 [3:53:23<00:00, 44.88s/pregunta]  \n"
     ]
    }
   ],
   "source": [
    "columns = [\n",
    "    \"pregunta\", \"respuesta esperada\", \"respuesta generada\", \n",
    "    \"accuracy\", \"faithfulness\", \n",
    "    \"groundedness\", \"relevance\"\n",
    "]\n",
    "\n",
    "eval_rag = []\n",
    "eval_rag_rr = []\n",
    "\n",
    "sleep = 2\n",
    "\n",
    "for index, row in tqdm(test_sample.iterrows(), desc=\"Evaluando el RAG\", total=len(test_sample), unit=\"pregunta\"):\n",
    "    pregunta = row[\"Pregunta\"]\n",
    "    respuesta = row[\"Respuesta\"]\n",
    "    contexto = row[\"Contexto\"]       \n",
    "\n",
    "    ### NORMAL ###\n",
    "\n",
    "    time.sleep(sleep) # Espera para evitar sobrecargar el servidor\n",
    "    respuesta_generada, docs_recuperados = get_response(pregunta, chatbot)  \n",
    "    # print(docs_recuperados)\n",
    "\n",
    "    evaluations = {\n",
    "        \"accuracy\": accuracy_prompt.format(respuesta_esperada=respuesta, respuesta_generada=respuesta_generada),\n",
    "        \"faithfulness\": faithfulness_prompt.format(documentos=docs_recuperados, respuesta=respuesta_generada),\n",
    "        \"groundedness\": groundedness_prompt.format(documentos=docs_recuperados, respuesta=respuesta_generada),\n",
    "        \"relevance\": relevance_prompt.format(pregunta=pregunta, respuesta=respuesta, documentos=docs_recuperados)\n",
    "    }        \n",
    "\n",
    "\n",
    "    ### RERANKING ###\n",
    "    time.sleep(sleep)\n",
    "    respuesta_generada_rr, docs_recuperados_rr = get_response_reranker(pregunta, chatbot)   \n",
    "    # print(docs_recuperados)\n",
    "\n",
    "    evaluations_rr = {\n",
    "        \"accuracy\": accuracy_prompt.format(respuesta_esperada=respuesta, respuesta_generada=respuesta_generada_rr),\n",
    "        \"faithfulness\": faithfulness_prompt.format(documentos=docs_recuperados_rr, respuesta=respuesta_generada_rr),\n",
    "        \"groundedness\": groundedness_prompt.format(documentos=docs_recuperados_rr, respuesta=respuesta_generada_rr),\n",
    "        \"relevance\": relevance_prompt.format(pregunta=pregunta, respuesta=respuesta, documentos=docs_recuperados_rr)\n",
    "    } \n",
    "\n",
    "    try:\n",
    "\n",
    "        eval_row = {\n",
    "            \"pregunta\": pregunta,\n",
    "            \"respuesta esperada\": respuesta,\n",
    "            \"respuesta generada\": respuesta_generada            \n",
    "        }\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            time.sleep(sleep) \n",
    "            chain = llm | StrOutputParser()\n",
    "            response = chain.invoke(evaluation)\n",
    "            match = re.search(r'Rating:\\s*(\\d+)', response)\n",
    "            score = int(match.group(1))\n",
    "            eval_row[criterion] = score\n",
    "        eval_rag.append(eval_row)\n",
    "\n",
    "        #####################\n",
    "\n",
    "        eval_row_rr = {\n",
    "            \"pregunta\": pregunta,\n",
    "            \"respuesta esperada\": respuesta,\n",
    "            \"respuesta generada\": respuesta_generada_rr            \n",
    "        }\n",
    "        for criterion, evaluation in evaluations_rr.items():\n",
    "            time.sleep(sleep) \n",
    "            chain = llm | StrOutputParser()\n",
    "            response = chain.invoke(evaluation)\n",
    "            match = re.search(r'Rating:\\s*(\\d+)', response)\n",
    "            score = int(match.group(1))\n",
    "            eval_row_rr[criterion] = score\n",
    "        eval_rag_rr.append(eval_row_rr)\n",
    "            \n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87930c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rag_df = pd.DataFrame(eval_rag, columns=columns)\n",
    "eval_rag_df.to_csv(f\"eval/eval_{search_type}.csv\", index=False)\n",
    "\n",
    "# print(\"Resultados RAG\")\n",
    "# eval_rag_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51fb75be",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rag_df_rr = pd.DataFrame(eval_rag_rr, columns=columns)\n",
    "eval_rag_df_rr.to_csv(f\"eval/eval_{search_type}_rr.csv\", index=False)\n",
    "\n",
    "# print(\"Resultados RAG + rerank\")\n",
    "# eval_rag_df_rr.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
