{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e687c5ad",
   "metadata": {},
   "source": [
    "# Evaluación de nuestro RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3bb1ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from classes.chatbot import Chatbot\n",
    "from classes.LLM import LLM, EMBEDDING\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ceb92",
   "metadata": {},
   "source": [
    "El modelo que vamos a usar para las evaluaciones y nuestro RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a4eeebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo de lenguaje que usaremos para evaluar\n",
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-small-latest\", \n",
    "    mistral_api_key=MISTRAL_API_KEY,\n",
    "    temperature=0, \n",
    "    random_seed=12345\n",
    ")\n",
    "\n",
    "search_type = \"similarity\" # [\"similarity\", \"mmr\", \"tfidf\", \"bm25\", \"grafo\"]\n",
    "chroma_directory = \"chroma/nomic_512\"  \n",
    "\n",
    "chatbot = Chatbot(\n",
    "    # language_model=LLM.llama_3_2_3B.value,   # modelo de lenguaje a testear\n",
    "    chroma_directory=chroma_directory,      # directorio de la base de datos\n",
    "    search_type=search_type,                # tipo de búsqueda en la base de datos\n",
    "    embedding_model=EMBEDDING.NOMIC,         # misma funcion de embedding que use la base de datos\n",
    "    k=10,                                   # documentos recuperados de la base de datos\n",
    "    top_n=5                                 # documentos recuperados por el reranker\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f21db7d",
   "metadata": {},
   "source": [
    "Cargamos una muestra de nuestro dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c37cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"eval/test - preguntas_mistral_plus.csv\", on_bad_lines=\"skip\")\n",
    "df_shuffle = df.sample(frac=1, random_state=12345)\n",
    "# test_sample = df_shuffle.head(1)\n",
    "test_sample = df\n",
    "# df_shuffle.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180055f9",
   "metadata": {},
   "source": [
    "## Configuración de los agentes críticos\n",
    "\n",
    "- **Accuracy**: Grado de precisión de la respuesta generada\n",
    "- **Faithfulness**: Si la respuesta se basa exclusivamente en los documentos recuperados\n",
    "- **Groundedness**: Si la pregunta puede ser respondida con el contexto aportado\n",
    "- **Relevance**: Si los docs recuperados tienen que ver con la pregunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcd0fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_prompt = \"\"\"\n",
    "Eres un agente crítico experto en evaluación de precisión de respuestas. \n",
    "Evalúa la precisión de la respuesta generada en comparación con la respuesta esperada. \n",
    "\n",
    "Criterios de evaluación:\n",
    "- 5: La respuesta generada incluye toda la información esencial de la respuesta esperada, incluso si es más extensa o utiliza otras palabras.\n",
    "- 4: La respuesta generada incluye casi toda la información importante, con leves omisiones o diferencias menores que no afectan el significado general.\n",
    "- 3: La respuesta generada cubre parte significativa del contenido, pero falta información importante o hay errores leves.\n",
    "- 2: La respuesta generada solo refleja una parte limitada del contenido de la respuesta esperada y tiene omisiones o distorsiones relevantes.\n",
    "- 1: La respuesta generada tiene muy poca relación con la respuesta esperada, presenta errores graves u omisiones importantes.\n",
    "- 0: La respuesta generada no contiene información relevante de la respuesta esperada o está completamente incorrecta.\n",
    "\n",
    "Recuerda: una redacción diferente o más extensa no debe penalizar si toda la información está presente.\n",
    "\n",
    "respuesta esperada: {respuesta_esperada}\n",
    "\n",
    "respuesta generada: {respuesta_generada}\n",
    "\n",
    "Responde solo con tu evaluación en el siguiente formato:\n",
    "\n",
    "Respuesta:::\n",
    "Rating: (tu evaluación del 0 al 5)\n",
    "Respuesta:::\n",
    "\"\"\"\n",
    "\n",
    "faithfulness_prompt = \"\"\"\n",
    "Tu tarea es evaluar si la respuesta generada usa EXCLUSIVAMENTE información contenida en los documentos proporcionados, sin añadir datos externos, suposiciones infundadas ni invenciones.\n",
    "\n",
    "Criterios de evaluación:\n",
    "5 - Totalmente basada en los documentos, sin añadir nada externo.\n",
    "4 - Principalmente basada, con pequeñas generalizaciones o frases neutras.\n",
    "3 - Incluye inferencias plausibles pero no explícitas en los documentos.\n",
    "2 - Mezcla partes basadas en los documentos con invenciones o suposiciones.\n",
    "1 - Mayormente inventada, aunque con algún detalle correcto.\n",
    "0 - Completamente inventada, contradictoria o irrelevante.\n",
    "\n",
    "Aquí tienes los documentos recuperados y la respuesta generada por el sistema RAG.\n",
    "\n",
    "Documentos recuperados: {documentos}\n",
    "\n",
    "Respuesta generada: {respuesta}\n",
    "\n",
    "Responde solo con tu evaluación en el siguiente formato:\n",
    "\n",
    "Respuesta:::\n",
    "Rating: (tu evaluación del 0 al 5)\n",
    "Respuesta:::\n",
    "\"\"\"\n",
    "\n",
    "groundedness_prompt = \"\"\"\n",
    "Tu tarea es evaluar en qué medida la respuesta está respaldada por evidencia específica de los documentos proporcionados.\n",
    "\n",
    "Criterios de evaluación:\n",
    "5 - Usa varias evidencias clave con precisión y contexto adecuado.\n",
    "4 - Usa al menos una evidencia clave de forma clara y correcta.\n",
    "3 - Se refiere a los documentos de manera vaga o genérica.\n",
    "2 - Uso superficial, indirecto o débilmente relevante de los documentos.\n",
    "1 - Menciona información incorrecta o mal interpretada del documento.\n",
    "0 - No hace uso de los documentos; ignora la evidencia disponible.\n",
    "\n",
    "Aquí tienes los documentos recuperados y la respuesta generada por el sistema RAG.\n",
    "\n",
    "Documentos recuperados: {documentos}\n",
    "\n",
    "Respuesta generada: {respuesta}\n",
    "\n",
    "Responde solo con tu evaluación en el siguiente formato:\n",
    "\n",
    "Respuesta:::\n",
    "Rating: (tu evaluación del 0 al 5)\n",
    "Respuesta:::\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "relevance_prompt = \"\"\"\n",
    "Tu tarea es evaluar qué tan relevantes son los documentos para responder la pregunta.\n",
    "\n",
    "Criterios de evaluación:\n",
    "5 - Abordan directamente el núcleo de la pregunta\n",
    "4 - Cubren aspectos importantes pero no todos\n",
    "3 - Proporcionan contexto general útil\n",
    "2 - Tienen conexión tangencial con el tema\n",
    "1 - Mínima relación con la pregunta\n",
    "0 - Completamente irrelevantes\n",
    "\n",
    "Aquí tienes la pregunta y los documentos recuperados.\n",
    "\n",
    "Pregunta: {pregunta}\n",
    "\n",
    "Respuesta esperada: {respuesta}\n",
    "\n",
    "Documentos recuperados: {documentos}\n",
    "\n",
    "Responde solo con tu evaluación en el siguiente formato:\n",
    "\n",
    "Respuesta:::\n",
    "Rating: (tu evaluación del 0 al 5)\n",
    "Respuesta:::\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3341c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(question: str, chatbot: Chatbot):\n",
    "    results = chatbot.get_retriever().invoke(question)\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in results]) \n",
    "    \n",
    "    response = chatbot.answer_query2(question, context)\n",
    "\n",
    "    return response, context\n",
    "\n",
    "def get_response_reranker(question: str, chatbot: Chatbot):\n",
    "    results = chatbot.get_compression_retriever().invoke(question)\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in results]) \n",
    "    \n",
    "    response = chatbot.answer_query2(question, context)\n",
    "\n",
    "    return response, context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b35e32",
   "metadata": {},
   "source": [
    "Evaluación del RAG + Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d333a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando el RAG: 100%|██████████| 1/1 [00:41<00:00, 41.77s/pregunta]\n"
     ]
    }
   ],
   "source": [
    "columns = [\n",
    "    \"pregunta\", \"respuesta esperada\", \"respuesta generada\", \n",
    "    \"accuracy\", \"faithfulness\", \n",
    "    \"groundedness\", \"relevance\"\n",
    "]\n",
    "\n",
    "eval_rag = []\n",
    "eval_rag_rr = []\n",
    "\n",
    "sleep = 2\n",
    "\n",
    "for index, row in tqdm(test_sample.iterrows(), desc=\"Evaluando el RAG\", total=len(test_sample), unit=\"pregunta\"):\n",
    "    pregunta = row[\"Pregunta\"]\n",
    "    respuesta = row[\"Respuesta\"]\n",
    "    contexto = row[\"Contexto\"]       \n",
    "\n",
    "    ### NORMAL ###\n",
    "\n",
    "    time.sleep(sleep) # Espera para evitar sobrecargar el servidor\n",
    "    respuesta_generada, docs_recuperados = get_response(pregunta, chatbot)  \n",
    "    # print(docs_recuperados)\n",
    "\n",
    "    evaluations = {\n",
    "        \"accuracy\": accuracy_prompt.format(respuesta_esperada=respuesta, respuesta_generada=respuesta_generada),\n",
    "        \"faithfulness\": faithfulness_prompt.format(documentos=docs_recuperados, respuesta=respuesta_generada),\n",
    "        \"groundedness\": groundedness_prompt.format(documentos=docs_recuperados, respuesta=respuesta_generada),\n",
    "        \"relevance\": relevance_prompt.format(pregunta=pregunta, respuesta=respuesta, documentos=docs_recuperados)\n",
    "    }        \n",
    "\n",
    "\n",
    "    ### RERANKING ###\n",
    "    time.sleep(sleep)\n",
    "    respuesta_generada_rr, docs_recuperados_rr = get_response_reranker(pregunta, chatbot)   \n",
    "    # print(docs_recuperados)\n",
    "\n",
    "    evaluations_rr = {\n",
    "        \"accuracy\": accuracy_prompt.format(respuesta_esperada=respuesta, respuesta_generada=respuesta_generada_rr),\n",
    "        \"faithfulness\": faithfulness_prompt.format(documentos=docs_recuperados_rr, respuesta=respuesta_generada_rr),\n",
    "        \"groundedness\": groundedness_prompt.format(documentos=docs_recuperados_rr, respuesta=respuesta_generada_rr),\n",
    "        \"relevance\": relevance_prompt.format(pregunta=pregunta, respuesta=respuesta, documentos=docs_recuperados_rr)\n",
    "    } \n",
    "\n",
    "    try:\n",
    "\n",
    "        eval_row = {\n",
    "            \"pregunta\": pregunta,\n",
    "            \"respuesta esperada\": respuesta,\n",
    "            \"respuesta generada\": respuesta_generada            \n",
    "        }\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            time.sleep(sleep) \n",
    "            chain = llm | StrOutputParser()\n",
    "            response = chain.invoke(evaluation)\n",
    "            match = re.search(r'Rating:\\s*(\\d+)', response)\n",
    "            score = int(match.group(1))\n",
    "            eval_row[criterion] = score\n",
    "        eval_rag.append(eval_row)\n",
    "\n",
    "        #####################\n",
    "\n",
    "        eval_row_rr = {\n",
    "            \"pregunta\": pregunta,\n",
    "            \"respuesta esperada\": respuesta,\n",
    "            \"respuesta generada\": respuesta_generada_rr            \n",
    "        }\n",
    "        for criterion, evaluation in evaluations_rr.items():\n",
    "            time.sleep(sleep) \n",
    "            chain = llm | StrOutputParser()\n",
    "            response = chain.invoke(evaluation)\n",
    "            match = re.search(r'Rating:\\s*(\\d+)', response)\n",
    "            score = int(match.group(1))\n",
    "            eval_row_rr[criterion] = score\n",
    "        eval_rag_rr.append(eval_row_rr)\n",
    "            \n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "###############################\n",
    "eval_rag_df = pd.DataFrame(eval_rag, columns=columns)\n",
    "eval_rag_df.to_csv(f\"eval/eval_{search_type}.csv\", index=False)\n",
    "eval_rag_df_rr = pd.DataFrame(eval_rag_rr, columns=columns)\n",
    "eval_rag_df_rr.to_csv(f\"eval/eval_{search_type}_rr.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
